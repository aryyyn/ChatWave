{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.layers import Conv1D, Bidirectional, LSTM, Dense, Input, Dropout\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import SpatialDropout1D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from collections import Counter\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Negative</td>\n",
       "      <td>upset update facebook texting might cry result...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Negative</td>\n",
       "      <td>dived many time ball managed save 50 rest go b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Negative</td>\n",
       "      <td>whole body feel itchy like fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Negative</td>\n",
       "      <td>behaving mad see</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Negative</td>\n",
       "      <td>whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                            message\n",
       "0  Negative  upset update facebook texting might cry result...\n",
       "1  Negative  dived many time ball managed save 50 rest go b...\n",
       "2  Negative                    whole body feel itchy like fire\n",
       "3  Negative                                   behaving mad see\n",
       "4  Negative                                         whole crew"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/cleaned_text_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data = train_test_split(df,test_size=0.2,random_state=11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1279999"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_length = train_data['message'].apply(len).max()\n",
    "# # print(\"Maximum message length:\", max_length)\n",
    "# # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame(train_data)\n",
    "test_data = pd.DataFrame(test_data)\n",
    "train_data['message'] = train_data['message'].astype(str)\n",
    "train_data['sentiment'] = train_data['sentiment'].astype(str)\n",
    "test_data['message'] = test_data['message'].astype(str)\n",
    "test_data['sentiment'] = test_data['sentiment'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 20000\n",
    "oov_token = \"unknown\"\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=oov_token)\n",
    "tokenizer.fit_on_texts(train_data['message'])\n",
    "\n",
    "tokenizer_json = tokenizer.to_json()\n",
    "with open('tokenizer.json', 'w') as json_file:\n",
    "    json_file.write(tokenizer_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5590, 390, 5590, 5590, 123, 304, 5590, 5590, 9]]\n"
     ]
    }
   ],
   "source": [
    "print(str(tokenizer.texts_to_sequences(['There seems to be something wrong with you today'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid= train_test_split(train_data['message'].tolist(),\n",
    "                                                      train_data['sentiment'].tolist(),\n",
    "                                                      test_size=0.1,\n",
    "                                                      stratify = train_data['sentiment'].tolist(),\n",
    "                                                      random_state=8)\n",
    "\n",
    "#further divided traindata into train data and validation data (validation data will just be used to make sure that the model is training correctly.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data len:1151999\n",
      "Class distributionCounter({'Positive': 576360, 'Negative': 575639})\n",
      "Valid data len:128000\n",
      "Class distributionCounter({'Positive': 64040, 'Negative': 63960})\n"
     ]
    }
   ],
   "source": [
    "print('Train data len:'+str(len(X_train)))\n",
    "print('Class distribution'+str(Counter(y_train)))\n",
    "print('Valid data len:'+str(len(X_valid)))\n",
    "print('Class distribution'+ str(Counter(y_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1151999, 50)\n",
      "(128000, 50)\n",
      "(320000, 50)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "valid_sequences = tokenizer.texts_to_sequences(X_valid)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data['message'].tolist())\n",
    "\n",
    "X_train = pad_sequences(train_sequences, padding='post', maxlen=50)\n",
    "x_valid = pad_sequences(valid_sequences, padding='post', maxlen=50)\n",
    "x_test = pad_sequences(test_sequences, padding='post', maxlen=50)\n",
    "\n",
    "\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "x_valid = np.array(x_valid)\n",
    "x_test = np.array(x_test)\n",
    "print(X_train.shape)\n",
    "print(x_valid.shape)\n",
    "print(x_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-09 10:39:44.394397: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 230399800 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "train_labels = le.fit_transform(y_train)\n",
    "train_labels = np.asarray( tf.keras.utils.to_categorical(train_labels))\n",
    "\n",
    "valid_labels = le.transform(y_valid)\n",
    "valid_labels = np.asarray( tf.keras.utils.to_categorical(valid_labels))\n",
    "\n",
    "test_labels = le.transform(test_data['sentiment'].tolist())\n",
    "test_labels = np.asarray(tf.keras.utils.to_categorical(test_labels))\n",
    "\n",
    "\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train,train_labels))\n",
    "valid_ds = tf.data.Dataset.from_tensor_slices((x_valid,valid_labels))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test,test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4201, 1356, 5590,  105,   40,    4,  223, 1856,  154,   58,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0], dtype=int32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 2\n"
     ]
    }
   ],
   "source": [
    "# Assuming train_data['sentiment'] contains all possible classes\n",
    "le = LabelEncoder()\n",
    "le.fit(train_data['sentiment'].tolist())\n",
    "\n",
    "# Transform labels\n",
    "train_labels = le.transform(y_train)\n",
    "train_labels = np.asarray(tf.keras.utils.to_categorical(train_labels))\n",
    "\n",
    "valid_labels = le.transform(y_valid)\n",
    "valid_labels = np.asarray(tf.keras.utils.to_categorical(valid_labels))\n",
    "\n",
    "test_labels = le.transform(test_data['sentiment'].tolist())\n",
    "test_labels = np.asarray(tf.keras.utils.to_categorical(test_labels))\n",
    "\n",
    "# Verify the number of classes\n",
    "num_classes = len(le.classes_)\n",
    "print(\"Number of classes:\", num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aryn/.local/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_7\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_7\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d_6          │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_6 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_6 (\u001b[38;5;33mConv1D\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d_6          │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "number_of_classes = 2  \n",
    "\n",
    "max_features = 20000\n",
    "embedding_dim = 64\n",
    "sequence_length = 50\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(max_features + 1, embedding_dim, input_length=sequence_length,\n",
    "                                    embeddings_regularizer=regularizers.l2(0.0005)))\n",
    "\n",
    "model.add(tf.keras.layers.Conv1D(128, 3, activation='relu',\n",
    "                                 kernel_regularizer=regularizers.l2(0.0005),\n",
    "                                 bias_regularizer=regularizers.l2(0.0005)))\n",
    "\n",
    "model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "\n",
    "model.add(tf.keras.layers.Dense(number_of_classes, activation='sigmoid',\n",
    "                                kernel_regularizer=regularizers.l2(0.001),\n",
    "                                bias_regularizer=regularizers.l2(0.001)))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False), \n",
    "              optimizer='Nadam', \n",
    "              metrics=[\"CategoricalAccuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m9000/9000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m765s\u001b[0m 85ms/step - CategoricalAccuracy: 0.7544 - loss: 0.5606 - val_CategoricalAccuracy: 0.7652 - val_loss: 0.5397\n",
      "Epoch 2/20\n",
      "\u001b[1m9000/9000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m786s\u001b[0m 87ms/step - CategoricalAccuracy: 0.7636 - loss: 0.5439 - val_CategoricalAccuracy: 0.7695 - val_loss: 0.5350\n",
      "Epoch 3/20\n",
      "\u001b[1m9000/9000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m791s\u001b[0m 88ms/step - CategoricalAccuracy: 0.7665 - loss: 0.5399 - val_CategoricalAccuracy: 0.7705 - val_loss: 0.5319\n",
      "Epoch 4/20\n",
      "\u001b[1m9000/9000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m800s\u001b[0m 89ms/step - CategoricalAccuracy: 0.7665 - loss: 0.5387 - val_CategoricalAccuracy: 0.7691 - val_loss: 0.5344\n",
      "Epoch 5/20\n",
      "\u001b[1m9000/9000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m792s\u001b[0m 88ms/step - CategoricalAccuracy: 0.7670 - loss: 0.5384 - val_CategoricalAccuracy: 0.7703 - val_loss: 0.5322\n",
      "Epoch 6/20\n",
      "\u001b[1m9000/9000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m775s\u001b[0m 86ms/step - CategoricalAccuracy: 0.7667 - loss: 0.5383 - val_CategoricalAccuracy: 0.7707 - val_loss: 0.5316\n",
      "Epoch 7/20\n",
      "\u001b[1m9000/9000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m781s\u001b[0m 87ms/step - CategoricalAccuracy: 0.7670 - loss: 0.5383 - val_CategoricalAccuracy: 0.7702 - val_loss: 0.5307\n",
      "Epoch 8/20\n",
      "\u001b[1m9000/9000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m820s\u001b[0m 89ms/step - CategoricalAccuracy: 0.7667 - loss: 0.5383 - val_CategoricalAccuracy: 0.7705 - val_loss: 0.5320\n",
      "Epoch 9/20\n",
      "\u001b[1m9000/9000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m821s\u001b[0m 91ms/step - CategoricalAccuracy: 0.7667 - loss: 0.5385 - val_CategoricalAccuracy: 0.7710 - val_loss: 0.5316\n",
      "Epoch 10/20\n",
      "\u001b[1m9000/9000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m820s\u001b[0m 91ms/step - CategoricalAccuracy: 0.7670 - loss: 0.5383 - val_CategoricalAccuracy: 0.7705 - val_loss: 0.5326\n",
      "Epoch 11/20\n",
      "\u001b[1m9000/9000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m825s\u001b[0m 92ms/step - CategoricalAccuracy: 0.7667 - loss: 0.5381 - val_CategoricalAccuracy: 0.7708 - val_loss: 0.5320\n",
      "Epoch 12/20\n",
      "\u001b[1m9000/9000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m831s\u001b[0m 92ms/step - CategoricalAccuracy: 0.7668 - loss: 0.5380 - val_CategoricalAccuracy: 0.7704 - val_loss: 0.5327\n",
      "Epoch 13/20\n",
      "\u001b[1m9000/9000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m868s\u001b[0m 93ms/step - CategoricalAccuracy: 0.7671 - loss: 0.5382 - val_CategoricalAccuracy: 0.7701 - val_loss: 0.5319\n",
      "Epoch 14/20\n",
      "\u001b[1m9000/9000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m841s\u001b[0m 93ms/step - CategoricalAccuracy: 0.7667 - loss: 0.5382 - val_CategoricalAccuracy: 0.7695 - val_loss: 0.5326\n",
      "Epoch 15/20\n",
      "\u001b[1m9000/9000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m841s\u001b[0m 93ms/step - CategoricalAccuracy: 0.7671 - loss: 0.5381 - val_CategoricalAccuracy: 0.7706 - val_loss: 0.5321\n",
      "Epoch 16/20\n",
      "\u001b[1m9000/9000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m844s\u001b[0m 94ms/step - CategoricalAccuracy: 0.7671 - loss: 0.5379 - val_CategoricalAccuracy: 0.7711 - val_loss: 0.5312\n",
      "Epoch 17/20\n",
      "\u001b[1m9000/9000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m844s\u001b[0m 94ms/step - CategoricalAccuracy: 0.7668 - loss: 0.5383 - val_CategoricalAccuracy: 0.7702 - val_loss: 0.5334\n",
      "Epoch 18/20\n",
      "\u001b[1m9000/9000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m861s\u001b[0m 94ms/step - CategoricalAccuracy: 0.7670 - loss: 0.5385 - val_CategoricalAccuracy: 0.7701 - val_loss: 0.5321\n",
      "Epoch 19/20\n",
      "\u001b[1m9000/9000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m844s\u001b[0m 94ms/step - CategoricalAccuracy: 0.7668 - loss: 0.5383 - val_CategoricalAccuracy: 0.7710 - val_loss: 0.5308\n",
      "Epoch 20/20\n",
      "\u001b[1m9000/9000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m847s\u001b[0m 94ms/step - CategoricalAccuracy: 0.7669 - loss: 0.5382 - val_CategoricalAccuracy: 0.7707 - val_loss: 0.5327\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "history = model.fit(train_ds.shuffle(2000).batch(128),\n",
    "                    epochs= epochs ,\n",
    "                    validation_data=valid_ds.batch(128),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('sentimentmodel.keras')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('sentimentmodel.h5') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json \n",
    "tokenizer = Tokenizer(num_words=max_features)  \n",
    "json_string = tokenizer.to_json()\n",
    "with open('tokenizer.json', 'w') as outfile:\n",
    "    outfile.write(json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.legacy.preprocessing.text.Tokenizer at 0x7fd712d10520>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate predictions for all samples\n",
      "\u001b[1m10000/10000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 4ms/step\n",
      "[[0.4569431  0.54305685]\n",
      " [0.4569431  0.54305685]\n",
      " [0.4569431  0.54305685]\n",
      " ...\n",
      " [0.4569431  0.54305685]\n",
      " [0.4569431  0.54305685]\n",
      " [0.4569431  0.54305685]]\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
